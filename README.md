# Parallels_HW
В данном репозитории будут представлены лабораторные работы по курсу Параллельное программирование, ММФ, НГУ

# OpenMP_SLAU
Решение СЛАУ с плотной матрицей итерационным методом Гаусса.

    Для верификации метода генерировалась матрица, решение которой известно - на диагонали значения n + 1, где n - размер матрицы, 
остальные значения - единицы. Вектор решений для такой матрицы полностью заполнен единицами.

Расчет был проведен на ПК Apple MacBook Air 2022 
Процессор M2 - 4 производительных ядер и 4 энергоэффективных (гибридная архитектура), 1 поток на ядро.

Результаты (время, ускорение, эффективность) - https://docs.google.com/spreadsheets/d/1W31IW7T3Pzd-LqoeukDTXBTbZXZnTd2NoBF0d5Day3s/edit?gid=0#gid=0.

Итоги: 
- видно, что на наименьшем размере вектора наименьшая эффективность (слабое ускорение), что связано с расходами на "общение" параллельных процессов.
- На гибридном процессоре с 4 производительными и 4 энергоэффективными ядрами использование всех 8 потоков замедляет вычисления из-за дисбаланса нагрузки, поскольку быстрые ядра вынуждены ждать медленные, тогда как 4 потока на производительных ядрах работают оптимально.


# MPI_SLAU
Также как и для OpenMP_SLAU, было проведено решение СЛАУ итерационным методом Гаусса. 

Для честного сравнения методов пралась та же самая матрица, что и в случае OpenMP

ПК - Apple MacBook Air 2022, Процессор M2 - 4 производительных ядер и 4 эффективных, 1 поток на ядро.

Результаты (время, ускорение, эффективность) - https://docs.google.com/spreadsheets/d/1W31IW7T3Pzd-LqoeukDTXBTbZXZnTd2NoBF0d5Day3s/edit?gid=1808621195#gid=1808621195

Итоги: так же, как и в случае openMP наблюдается рост эффективности при росте размера исходных матриц - уменьшается доля времени на накладные расходы

# CUDA_SLAU
Аналогичное решение СЛАУ с написанием кода на CUDA для вычисления на gpu. 

Вычисление проводилось на кластере ИТ СО РАН "Каскад" на видеокарте NVIDIA A100.

Результаты - (время, ускорение, эффективность) - https://docs.google.com/spreadsheets/d/1W31IW7T3Pzd-LqoeukDTXBTbZXZnTd2NoBF0d5Day3s/edit?gid=823471207#gid=823471207

Итоги: - ускорение падает с ростом blockSize

# MATRIX_MULTIPLICATION
В рамках данного задания проводилось вычисление произведения матриц в MPI на 2D решетке (https://ssd.sscc.ru/sites/default/files/content/attach/343/parallel_lab3_2020.pdf)

Расчет был проведен на ПК Apple MacBook Air 2022 
Процессор M2 - 4 производительных ядер и 4 энергоэффективных (гибридная архитектура), 1 поток на ядро.

Чтобы не сталкиваться с дисбалансом нагрузки, вычисления проводились на 1, 2 и 4 производительных ядрах

Результаты (время, ускорение, эффективность) - https://docs.google.com/spreadsheets/d/1W31IW7T3Pzd-LqoeukDTXBTbZXZnTd2NoBF0d5Day3s/edit?gid=148107311#gid=148107311

Итоги: - виден заметный прирост эффективности параллельных расчетов, в сравнении с итерационным решением СЛАУ, что можно объяснить тем, что вычисления перемножений блоков матриц не зависят от таких параметров обусловленность матрицы, эффективность итерационного метода и тд, а лишь от размеров матриц и доступных вычислительных ресурсов, что обеспечивает идеальную масштабируемость и почти линейный рост производительности с увеличением числа потоков для больших матриц

# LOAD_BALANCING
Здесь реализована имитация распределённой многопоточной системы обработки заданий с использованием MPI и pthreads, что осуществляет гибридный параллелизм:
    - MPI используется для межпроцессного взаимодействия — процессы обмениваются заданиями.
    - pthreads используются внутри одного MPI-процесса для внутренней многопоточной обработки.

- Для проверки потокобезопасности было проведено несколько тестовых запусков программы на разном числе потоков (2-3-4), отсутствие дедлоков (зависаний программы) говорит о правильном использовании примитивов синхронизации (мьютексов) и атомарных операций. 
- Была определена масштабируемость (рост ускорения на разных количествах потоков), что и получилось, для 2, 3 и 4 потоков ускорение получилось 1.65, 2.48, 2.6 соответственно. Нелинейный рост времени можно списать на накладные расходы
- Для тестирования неравномерной загрузки системы, изначально все задачи давались нулевому рангу 

```cpp
if (worldRank == 0) {
    for (int i = 0; i < 5; ++i) {
        pthread_mutex_lock(&queueMutex);
        jobQueue.push(Job(2));
        pthread_mutex_unlock(&queueMutex);
    }
}
```

из лога можно видеть как нулевой процесс "делится" задачами 

```
[Rank 1] Fetcher: activated — seeking work...
[Rank 0] Responder: handled request #1 from rank 3 → sent job(2)
[Rank 3] Fetcher: got job(2) from rank 0
[Rank 3] Worker: processing job #1 (sleep=2)
[Rank 0] Responder: handled request #2 from rank 1 → sent job(2)
[Rank 0] Responder: handled request #3 from rank 2 → sent job(2)
```

Итоги: реализованная система демонстрирует эффективное сочетание межпроцессного и внутреннего многопоточного параллелизма, обеспечивая масштабируемость и устойчивость к неравномерной начальной загрузке за счёт корректной синхронизации и динамического распределения задач.